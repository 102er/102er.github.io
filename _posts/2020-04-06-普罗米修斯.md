---
layout: post
title: '普罗米修斯'
date: 2020-04-16
author: blossom
cover: 'http://on2171g4d.bkt.clouddn.com/jekyll-banner.png'
tags: 监控 普罗米修斯 prometheus
---

> 介绍普罗米修斯，搭建普罗米修斯

### 基础
Prometheus 由go语言编写而成，是一套开源的系统监控报警框架。它启发于 Google 的 borgmon 监控系统，由工作在 SoundCloud 的 google 前员工在 2012 年创建，作为社区开源项目进行开发，并于 2015 年正式发布。
随着kubernets在容器调度和管理上异常火热，Prometheus也成为kubernets容器监控的标配。
Prometheus特点：
* 强大的多维度数据模型和灵活的数据查询方式：
    * 所有采集的监控数据均以指标(metric)的形式保存在内置的时间序列数据库当中(TSDB)，每个时间序列由时间序列名称和标签的组合唯一标识；
    * 通过label标签实现多维度数据模型，通过label可以设置通过metric的不同的维度数据；
    * 通过监控指标关联多个label，来将监控数据进行任意维度的组合，并且提供简单的promQL查询方式，对外提供http接口，可以方便与第三方组件结合（比如：grafana）
* 易于管理：
    * 由go编写，编译后就是一个二进制文件，本身就是存储+监控架构不存在第三方依赖，唯一的部署环境因素就是磁盘空间；
    * 采用pull的模型采集数据，只有网络互通都能搭建Prometheus；
    * 支持静态配置+服务发现管理监控目标；
* 易于集成，提供了主流语言的客户端sdk，可以让应用程序快速接入Prometheus监控；
* 高效，单一的server可以处理数百万的 metrics；

### 组件详见：
基本原理就是通过http周期性抓取被监控组件的状态，任意组件只要提供对应的http接口并且符合promethues定义的数据格式，就可以接入Prometheus监控。
![prometheus架构图](https://blossom102er.github.io/assets/img/prometheus.png)
主要包含Prometheus server Pushgateway PromQL Alertmanager等组件。

#### Prometheus server
prometheus的核心组件，负责数据收集和存储。
* 定期从配置好的jobs或者exporters拉取metrics，或者从pushgateway接收metrics；
* Prometheus server会运行定义好的alert.rules 记录新的时间序列或者向alertmanager推送告警

#### Pushgateway
推送网关，接收agent server推送的告警，供Prometheus server拉取数据；
主要应用于短生命周期的jobs采集，因为短周期的jobs，可能在server还没来得及pull就已经消失了。

#### Alertmanager
告警生成以及通知告警组件，接收Prometheus server推送的告警，根据配置文件，对告警进行处理以及通报；
具备去重功能，可以对重复的alerts进行去重。

#### PromQL
数据查询语言，grafana或者其他api通过promQL来查询使用数据。

### 浅谈设计
主要从prometheus的指标设计，数据采集，数据处理，数据存储以及数据查询介简单介绍下promethues的设计。
#### 数据指标
Prometheus指标有统一的数据格式，如下：

    <metric name>{<label name> = <label value>,...}
    ### 数据示例
    # HELP flow_decoder_count Decoder processed count.
    # TYPE flow_decoder_count counter
    flow_decoder_count{name="sFlow",worker="0"} 1.853415e+06
    flow_decoder_count{name="sFlow",worker="1"} 1.853418e+06
    ### 指标名称 （metric name）
    指标名称，用于说明指标的含义，比如：flow_decoder_count代表解析流量包的总数；
    ### 标签 （label）
    体现指标维度特征，用于过滤和聚合。它通过标签名和标签值这种键值对形式，形成多维度。
    比如：对于指标flow_decoder_count，可以包含{name="sFlow",worker="0"}和{name="sFlow",worker="1"}两个标签。可以查询worker=0和worker=1解析sflow包的总量，或者通过flow_decoder_count{name="sFlow"}聚合查询所有worker解析sflow包的总数；
    
指标分类也是固定的，包含：Counter(计数器)、Gauge(仪表盘)、Histogram(直方图)和Summary(摘要)四类：

    ### Counter
    只增不减的计数器类型，具有很好的不相关性，不会因为机器重启而置为0；用于：接口请求数，任务完成数等。
    ### Gauge
    仪表盘，表征指标的实时变化情况，可增可减；用于：系统负载，内存使用等。
    ### Summary
    用于凸显数据的分布状况，由客户端分位，无需消耗服务端资源；
    ### Histogram
    反映某个区间的样本个数，通过{le="上边界"}指定这个范围内的样本个数；用于数据分组，比如耗时分布

#### 数据采集
promethues采用pull方式采集监控数据，由server端主动去拉取监控机数据。
为了兼容push方式，Prometheus提供了pushgateway组件，pushgateway组件接收客户端发送过来的数据，按照job和instance两个层级进行组织、支持数据的追加和删除。并且为防止数据丢失，还支持本地存储。
server通过服务发现方式去获取监控机的，promethues的服务发现方式主要是：静态文件配置和动态发现。
静态文件配置：适用于固定的监控环境，ip地址和统一的服务接口场景；当服务发生迁移、变更等情况需要修改目标配置文件，Prometheus默认5m会重新读取一次配置文件；（可以通过 kill -HUP pid的方式更新 or 手动触发reload接口）
动态发现：这种方式适合在云环境使用。Prometheus支持：kubernets、zk、consul等组件动态发现监控目标；
有了监控对象，promethues server 会为每一个采集点启用一个协程去定时请求监控机的metrics接口获取监控数据。

#### 数据处理
Prometheus支持数据处理，主要包括relabel，replace，keep，drop等操作，提供过滤数据或者修改样本的维度信息等功能。
promethues会从target中获取所有暴露的数据，在某些特定情况下，有些数据对promethues无用，如果存储了不仅浪费存储空间，还会降低系统的吞吐量，这时就可以设置keep或者drop机制:
* keep：保留所有匹配的标签数据
* drop：丢弃匹配的数据

<br>另外promethues还支持通过hash分区采集，通过对target地址计算hash值，取模匹配预定设定的值，达到过滤该promethues负责采集的target；

#### 数据存储
promethues自带时序数据库，将数据保存在本地，从而实现高性能读写，弊端就是非集群数据库，限制存储容量，无法保存大量的历史数据，为此promethues引入了远端存储；

##### 本地存储
promethues的tsdb时序数据库，主要通过block和wal实现：
![prometheus架构图](https://blossom102er.github.io/assets/img/prometheus-block.png)
    
    #block设计
        * TSDB将存储的监控数据按照时间分割成block，block的大小并不固定，按照设定的步长倍数递增；默认最小的block保存2小时监控数据；当数据量不断增长的时候，tsdb会将小的block合并成大的block，减少数据存储还可以减少内存中block的个数，便于数据检索；
        * 每个block都有全局唯一的名称，通过ULID原理生成，名称组成包含创建时间，方便block排序。
    #wal机制
        * WAL(预写日志)时关系型数据库中利用日志来实现事务性和持久性的一种技术，即在进行某个操作之前先将这件事记录下来，以便对数据进行回滚、重试等操作，保证数据的可靠性。
        * 为什么要引入这个机制？可以看下上面block的时间轴图，head block是存储在内存中的，如果服务异常，数据是会丢失的，加入wal机制，当Prometheus服务崩溃或者异常时，重启服务，会先启动多协程读取WAL，恢复之前的状态。
##### 远程存储
Prometheus并没有直接对接各种远端存储，而是定义了一套读写存储接口，并引入Adapter适配器，将Prometheus的读写请求转化为第三方远端存储接口，从而完成数据读写，整体架构如图：
![prometheus架构图](https://blossom102er.github.io/assets/img/prometheus-apapter.png)
Prometheus对adapter读请求是同步的，但是写请求则通过队列实现，主要是考虑后端存储写入性能以及Prometheus自身的写操作性能；队列的实现...
Prometheus通过"http post请求+protobuf编码"方式调用adapter的读写接口，详细可以阅读官方给的示例源码：documentatin/examples/remote_storage/remote_storage_adpater
就算配置了远端存储，本地的存储仍然也是有效的，so Prometheus设计了一个存储的汇聚接口fanout，首先执行primary写入，然后遍历写入每个远端。

#### 数据查询
Prometheus实现了一台自己的数据库查询语言：promQL；
promethues通过解析引擎将查询语句转化为query请求，然后通过时序数据库找到具体的数据块，在数据返回时在通过内置函数处理数据。

### 告警-alertmanager
promethues通过预先定义的global.evaluation_interval定时去执行配置的promQL，如果查询结果符合定义的告警规则，就会产生一条告警，但不会立即发出，而需要经过一个评估时间，如果在评估周期内每个周期都触发该条告警，则会向外发出这条告警。
alertmanger组件提供了告警处理的能力，主要功能包括：告警分组，告警抑制和告警静默。
* 告警分组：将多条告警合并一起发送
* 告警抑制：告警发出时，可以停止发送由此告警触发的其他错误告警
* 告警静默：一段时间内不发出重复的告警

alertmanager本身就是一个高可用架构实现，每个promethues都会配置多个alertmanager，避免单点故障。而alertManager通过以下两种方式实现告警的去重：
引入Gossip协议，保证每个alertManager的静默配置一致，从而保证每个alertManager都能静默告警，另外，告警在每个alertManager之间同步，通过gossip告知其他节点已经发出的告警，从而达到去重。
通过wait机制，将每个节点的wait时间调整不同时间，保证alertManager之间有足够时间完成的数据同步。

### exporter
所有向Prometheus提供监控样本数据的程序都可以成为expoter，主要负责收集数据，并暴露接口给Prometheus server定期采集；常用的exporter:
* node_exporter：采集服务器层面的运行指标；
* blackbox_exporter：黑盒监控解决方案，支持http，dns，tcp等网络协议；
* mysqld_exporter：mysql相关监控；
* client_goland：官方提供的库，不管node-exporter，blackbox-exporter等都有引用client_goland；

ps：官方提供了非常多exporter的实现，用户也可以根据自己的需要创建expoter程序。
参考连接：https://github.com/prometheus

#### 服务搭建
* 引入“联邦”机制，主要是为了拓展单个promethues的采集能力和处理能力；多个promethues节点组成一个层级联邦结构，上一层的节点负责定时从下一层节点获取数据并汇总；这种架构可以降低单给promethues的采集负载，而且通过联邦节点汇聚核心数据，可以降低本地存储的压力；
* thanos一套promethues高可用的解决方案，主要是解决promethues联邦集群缺少统一的全局视图和历史数据存储的问题。

具体集群相关的服务搭建，这边不展开，有兴趣的可以去了解一下，以上两种方式。

### 总结
promethues并非监控银弹，设计之初的定位：一款实时监控系统，只针对性能和可用性监控，并不具备日志监控等功能，并不能通过 Prometheus 解决所有监控问题；
采用本地存储，设计初衷就是保留短期的数据，并非针对大量的历史数据存储。
